---
title: "IRT models in NIMBLE"
author: "Sally Paganin"
bibliography: bibliography.bib
output: 
  pdf_document: default
  html_document:
    code_folding: show
link-citations: yes
subtitle: January 2020
biblio-style: apalike
---

## Intro and data simulation

This tutorial illustrate how to use NIMBLE for parametric and semiparametric 1PL and 2PL models, in support of the 

```{r generateData}

## Simulated data with unimodal latent abilities
set.seed(1)

# set the number of items I and persons P
nItems    <- 15
nPersons  <- 2000

# set the item and population parameters
lambda0     <- c(1 + runif(nItems,-0.5,0.5))
beta0     	<- seq(-3,3,length=nItems)

## Sum to zero constriants
lambda0 <- exp(log(lambda0) - mean(log(lambda0)))

## distribution of ability (bimodal)
etaMean1  <- -2
etaMean2  <- 2
etaSigma2 <- (1.25)^2


# generate thetas from a mixture and the I x P matrix of response probabilities
etaAbility  <- rnorm(nPersons/2, mean=etaMean1, sd=sqrt(etaSigma2))
etaAbility  <- c(etaAbility, rnorm(nPersons/2, mean=etaMean2, sd=sqrt(etaSigma2)))
term1       <- outer(etaAbility, lambda0)
term2       <- matrix(rep(lambda0*beta0, nPersons), nrow=nPersons,byrow=TRUE)
prob        <- plogis(term1-term2)  ### 1/(1 + exp(term.2 - term.1))
# generate the 0/1 responses Y as a matrix of Bernoulli draws

Y        <- ifelse(runif(nItems*nPersons) < prob, 1, 0)
```

## 1PL model

The one-parameter logistic model (1PL) assumes the presence of one parameter, $\beta_i$, for each item $i = 1, \ldots, I$ encoding the *difficulty* of the item. The parameter $\eta_j$ for $j = 1, \ldots, N$ represent the indivudual latent *ability* and in this example we assume that $\eta_j \stackrel{iid}{\sim} \mathcal{N}(0,1)$. This translates in nimble code


```{r 1PLmodel}
library(nimble)

## NIMBLE model code
code1PL <- nimbleCode({  
  for(i in 1:I) {
    for(j in 1:N) {
      y[j,i] ~ dbern(pi[j,i])
      logit(pi[j,i]) <-  eta[j] - beta[i]
    }
    beta[i] ~ dnorm(0, var = 3)
  }  
  
  for(j in 1:N) {
    eta[j] ~ dnorm(0, 1)
  }

})

## set model constants and data
constants <- list(I = ncol(Y), N = nrow(Y))
data <- list(y = Y)

## set inits
set.seed(1)
inits <- list(beta    = rnorm(constants$I, 0, 1),
              eta     = rnorm(constants$N, 0, 1))

## set monitors
monitors = c("beta", "eta")

## create and compile model
model1PL <- nimbleModel(code1PL, constants, data, inits)
cModel1PL <- compileNimble(model1PL)
```

### Building and running an MCMC to fit the model

This chunk create an MCMC configuration, build and compile the MCMC. 


```{r 1PLmodel_compile}
conf1PL <- configureMCMC(model1PL, monitors = monitors)

model1PLMCMC <- buildMCMC(conf1PL)
cModel1PLMCMC <- compileNimble(model1PLMCMC, project = model1PL)
```

Run the model and save posterior samples. 

```{r 1PLmodel_run}
system.time(samples1PL <- runMCMC(cModel1PLMCMC, niter = 5000, nburnin = 1000))
```

### Checking the results

We can obtain a summary of the estimates of item difficulties.

```{r}
betaCols <- grep("beta", colnames(samples1PL))
samplesSummary(samples1PL[, c(betaCols)])
```

## 2PL model 

The 2PL model generalize the 1PL model,\ considering an additional parameter, $\lambda_i$, for each item $i = 1, \ldots, I$, often referred as *discrimination* parameter. Typically the $\lambda_i$'s are assumed to be positive, so we choose independent lognormal distributions as priors.
 
In this example we use a Dirichlet Process (DP) mixture model to represent the distribution of ability. In particular we use a Chinese Restaurant Process reprensetation (CRP) that in NIMBLE can simply be defined via a `dCRP` distribution. Details about Bayesian nonparametric models in NIMBLE can be found in Chapter 10 of the user manual [@nimble-manual:2020].
[blog post](https://r-nimble.org/bayesian-nonparametric-models-in-nimble-part-1-density-estimation).

```{r semiparametric_2PLmodel}
##----------------------------------------##
code2PL <- nimbleCode({
  for(i in 1:I) {
    for(j in 1:N) {
      y[j, i] ~ dbern(pi[j, i])
      logit(pi[j, i]) <-  lambda[i]*(eta[j] - beta[i])
    }
  }  
  
  for(i in 1:I) { 
    log(lambda[i]) ~ dnorm(0.5, var = 0.5)   
    beta[i] ~ dnorm(0,  var = 2)
  } 
    
  ## DP mixture model for distribution of ability
  ## CRP representation
  zi[1:N] ~ dCRP(alpha, size = N)
  alpha ~ dgamma(a, b)  

  ## Mixture component parameter drawn from the base measure
  for(j in 1:N) {
    eta[j] ~ dnorm(mu[j], var = s2[j])  
    mu[j] <- muTilde[zi[j]]                 
    s2[j] <- s2Tilde[zi[j]]   
  }

  for(m in 1:M) {
    muTilde[m] ~ dnorm(0, var = s2_mu)
    s2Tilde[m] ~ dinvgamma(nu1, nu2)
  }
})

constants <- list(I= dim(Y)[2], N = dim(Y)[1], M = 50)

inits <- list(beta       = rnorm(constants$I, 0, 1),
              log_lambda = runif(constants$I, -1, 1),
              nu1 = 2.01, nu2 = 1.01, s2_mu = 2, 
              alpha   = 1, a = 1, b = 3)
scores 		<- apply(data$y, 1, sum)
Sscores 	<- (scores - mean(scores))/sd(scores)
inits$eta 	<- Sscores
inits$zi 	<- kmeans(Sscores, 3)$cluster
inits$lambda <- exp(inits$log_lambda)

monitors <- c("beta", "lambda",  "zi", "muTilde", "s2Tilde", "alpha")

## create and compile model
model2PL <- nimbleModel(code2PL, constants, data, inits)
cModel2PL <- compileNimble(model2PL)
```

### Building and running an MCMC to fit the model

```{r 2PLmodel_compile}
conf2PL <- configureMCMC(model2PL, monitors = monitors)

model2PLMCMC <- buildMCMC(conf2PL)
cModel2PLMCMC <- compileNimble(model2PLMCMC, project = model2PL)
```


```{r 2PLmodel_run}
system.time(samples2PL <- runMCMC(cModel2PLMCMC, niter = 10000, nburnin = 1000))

```
### Checking the results

Similarly to the previous case, we can obtain posterior summaries of the quantities of interest. Once again we would want to run the MCMC for longer. 

```{r 2PL_posterior_estimates}

betaCols   <- grep("beta", colnames(samples2PL))
lambdaCols <- grep("lambda", colnames(samples2PL))
# etaCols    <- grep("eta", colnames(samples2PL))

samplesSummary(samples2PL[, c(betaCols, lambdaCols)])
```

### Post-process posterior samples

In the pape

<!-- ```{r}
    
nItems <- dim(lambdaSamp)[2]
nSamp <- dim(lambdaSamp)[1]

locationSamp <- sapply(1:nSamp, function(x) sum(betaSamp[x,])/nItems)
scaleSamp    <- apply(lambdaSamp, 1, function(x) prod(x)^(-1/nItems))

betaSamp <- t(sapply(1:nSamp, function(x) (betaSamp[x, ] - locationSamp[x])/scaleSamp[x] ))
lambdaSamp <- t(apply(lambdaSamp, 1, function(x) x*(prod(x)^(-1/nItems)) ))

indicesEta <- seq(thinEta, nSamp, by = thinEta)

etaSampRescaled <- matrix(0, NROW(etaSamp), NCOL(etaSamp))
for(i in 1:dim(etaSamp)[1]){
  etaSampRescaled[i, ] <- (etaSamp[i, ] - locationSamp[indicesEta[i]])/scaleSamp[indicesEta[i]]
}
colnames(etaSampRescaled) <- colnames(etaSamp) 
etaSamp <- etaSampRescaled


```
 -->
### Generating samples from the posterior mixing distribution

In the semiparametric setting, it is possible to make full inference on the distribution of ability and its functionals. A computational approach to obtain the samples from the posterior of the mixing distribution has been presented in [@gelfand2002], a version of whose algorithm is implemented in NIMBLE in the function `getSamplesDPMeasure`.



# References
